
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Supervised machine learning &#8212; Applied Machine Learning, 2021</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'supervised';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="practicalities.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/ApplesAndOranges.png" class="logo__image only-light" alt="Applied Machine Learning, 2021 - Home"/>
    <script>document.write(`<img src="_static/ApplesAndOranges.png" class="logo__image only-dark" alt="Applied Machine Learning, 2021 - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="practicalities.html">
                    About the course
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Introduction.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="ReadingAndPlotting.html">2. Reading and plotting</a></li>
<li class="toctree-l1"><a class="reference internal" href="Preprocessing_and_feature_extraction.html">3. Preprocessing and feature extraction</a></li>
<li class="toctree-l1"><a class="reference internal" href="Subspace_Projections.html">4. Dimensionality reduction by Subspace projections</a></li>
<li class="toctree-l1"><a class="reference internal" href="Clustering.html">5. Unsupervised learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="SupervisedMachineLearningTerminology.html">6. Training process</a></li>




<li class="toctree-l1"><a class="reference internal" href="NearestNeighbors.html">11. Nearest Neighbours methods <a class="anchor" id="nearestneighbours"></a></a></li>
<li class="toctree-l1"><a class="reference internal" href="SupportVectorMachine.html">12. Support Vector Machine (SVM) <a class="anchor" id="supportvectormachine"></a></a></li>
<li class="toctree-l1"><a class="reference internal" href="DecisionTrees.html">13. Decision trees and forests <a class="anchor" id="dtaforests"></a></a></li>
<li class="toctree-l1"><a class="reference internal" href="Regression.html">14. Regression and regularisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="NeuralNetworks.html">15. Artificial Neural Networks (ANN)</a></li>
<li class="toctree-l1"><a class="reference internal" href="Learning_model_parameters.html">16. Model learning strategies</a></li>





<li class="toctree-l1"><a class="reference internal" href="NLP.html">22. Natural Language Processing</a></li>

<li class="toctree-l1"><a class="reference internal" href="NLP-UWB.html">24. Ultra Wide Band positioning literature analysis</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://notebooks.csc.fi/#/blueprint/d1fe6e08032e4c17a0f9e0e222414598/hub/user-redirect/git-pull?repo=https%3A//github.com/pevalisuo/AML.git&urlpath=tree/AML.git/book/supervised.ipynb&branch=master" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on JupyterHub"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="JupyterHub logo" src="_static/images/logo_jupyterhub.svg">
  </span>
<span class="btn__text-container">JupyterHub</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/supervised.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Supervised machine learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning">Machine learning <a class="anchor" id="ML"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-target-of-supervised-machine-learning">The target of supervised machine learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-validation-and-testing">Training, Validation and Testing <a class="anchor" id="TrainTestValidate"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-validation">Cross Validation <a class="anchor" id="crossvalidation"></a></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distance-metrics">Distance metrics <a class="anchor" id="distance"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling">Scaling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#then-scale-it-first">Then scale it first</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#categorial-features">Categorial features?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-encoding">One hot encoding  <a class="anchor" id="OneHotEncoding"></a></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nearest-neighbours-methods">Nearest Neighbours methods <a class="anchor" id="nearestneighbours"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#brute-force-implementation">Brute force implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pipelining">Pipelining</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-of-the-decision-boundaries">Visualization of the decision boundaries</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variations">Variations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nearest-centroid-classifier">Nearest Centroid Classifier</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#support-vector-machine-svm">Support Vector Machine (SVM) <a class="anchor" id="supportvectormachine"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-boundary">Decision boundary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-svm">Kernel SVM <a class="anchor" id="kernelsvm"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#non-linear-classes">Non-linear classes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#illustration-of-rbf">Illustration of RBF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-rbf-in-circular-data">Testing RBF in circular data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-trees-and-forests">Decision trees and forests <a class="anchor" id="dtaforests"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-tree">Decision tree <a class="anchor" id="decisiontrees"></a></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimize-the-tree-depth">Optimize the tree depth</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ensemble-methods">Ensemble methods <a class="anchor" id="ensemblemethods"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bagging">Bagging <a class="anchor" id="Bagging"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#randomized-trees">Randomized trees   <a class="anchor" id="RandomizedTrees"></a></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#extratrees-classifier-extremely-randomized-trees">Extratrees classifier (Extremely randomized trees)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#optimising-the-parameters-fo-the-extratrees-classifier">Optimising the parameters fo the Extratrees classifier</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#boosting">Boosting <a class="anchor" id="Boosting"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adaboost">Adaboost <a class="anchor" id="AdaBoost"></a></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-tree-boosting">Gradient Tree Boosting <a class="anchor" id="GBRT"></a></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#extreme-gradient-boosting-xgboost">Extreme Gradient Boosting (XGBoost) <a class="anchor" id="XGBoost"></a></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="supervised-machine-learning">
<h1>Supervised machine learning<a class="headerlink" href="#supervised-machine-learning" title="Link to this heading">#</a></h1>
<section id="machine-learning">
<h2>Machine learning <a class="anchor" id="ML"></a><a class="headerlink" href="#machine-learning" title="Link to this heading">#</a></h2>
<p><img alt="featureextraction_p.svg" src="_images/featureextraction_p.svg" /></p>
<ul class="simple">
<li><p>The purpose of the supervised machine learning is the teach an algorithm to repeat the work done by a reference method</p></li>
<li><p>This is usefull if the reference method is more expensive, slower or it has other problems</p></li>
<li><p>Often the reference method is a human observer, and it can be replaced with an algorithm</p></li>
</ul>
</section>
<section id="the-target-of-supervised-machine-learning">
<h2>The target of supervised machine learning<a class="headerlink" href="#the-target-of-supervised-machine-learning" title="Link to this heading">#</a></h2>
<p>Task is to find a function <span class="math notranslate nohighlight">\(f\)</span>, which predicts variable <span class="math notranslate nohighlight">\(y_i\)</span> based on <span class="math notranslate nohighlight">\(p\)</span> features <span class="math notranslate nohighlight">\(x_{i,j}\)</span>, where <span class="math notranslate nohighlight">\(i \in [0,N]\)</span> and <span class="math notranslate nohighlight">\(j \in [0,P]\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
   \begin{bmatrix}
   y_1 \\
   y_2 \\
   \vdots \\
   y_n
   \end{bmatrix}
   = f 
   \left( \begin{bmatrix}
     x_{11} &amp; x_{12} &amp; x_{13} &amp; \dots  &amp; x_{1p} \\
     x_{21} &amp; x_{22} &amp; x_{23} &amp; \dots  &amp; x_{2p} \\
     \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
     x_{n1} &amp; x_{n2} &amp; x_{n3} &amp; \dots  &amp; x_{np}
   \end{bmatrix} \right)
\end{split}\]</div>
</section>
<section id="training-validation-and-testing">
<h2>Training, Validation and Testing <a class="anchor" id="TrainTestValidate"></a><a class="headerlink" href="#training-validation-and-testing" title="Link to this heading">#</a></h2>
<p>Building predictive models requires following stages</p>
<ol class="arabic simple">
<li><p>Model building (training)</p></li>
<li><p>Model validation (often within an interation/optimisation loop)</p></li>
<li><p>Model testing (in the end)</p></li>
</ol>
<p>Important rules related to model building and testing</p>
<ol class="arabic simple">
<li><p>The model cannot be tested using training set, because that would lead to overfitting</p></li>
<li><p>Test set may not bet used many times, because then you would overfit to the test data</p></li>
</ol>
<p>Each stage requires data. The original data can be split in three different sets, one for each stage, but high quality labeled data is usually scarce resource, and in that cases slightly smarter method of using the data is needed.</p>
<section id="cross-validation">
<h3>Cross Validation <a class="anchor" id="crossvalidation"></a><a class="headerlink" href="#cross-validation" title="Link to this heading">#</a></h3>
<p><img alt="crossvalidation.svg" src="_images/crossvalidation.png" /></p>
<ul class="simple">
<li><p>Cross validation is an important technique to utilize the data more efficiently for all supervised training purposes</p></li>
<li><p>With cross validation, the training set is divided in N-folds.</p></li>
<li><p>At first (N-1) folds are used for training and 1 fold for validation</p></li>
<li><p>The process is repeated N times, until every sample has participated in training and validation sets</p></li>
<li><p>The final score is the average of all N scores</p></li>
</ul>
</section>
</section>
<section id="distance-metrics">
<h2>Distance metrics <a class="anchor" id="distance"></a><a class="headerlink" href="#distance-metrics" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Clustering, classification and regression assume that samples near each other, at close proximity, are more similar with each other than those which are farther away.</p></li>
</ul>
<p>But what do we mean by distance?</p>
<p>There are many distance metrics:</p>
<ul class="simple">
<li><p>Euclidean distance <span class="math notranslate nohighlight">\(r_{1,2}=\sqrt{(x_1 -x_2)^2 + (y_1 - y_2)^2 + (z_1 -z_2)^2}\)</span></p></li>
<li><p>Manhattan distance <span class="math notranslate nohighlight">\(r_{1,2}=|x_1-x_2| + |y_1-y_2| + |z_1-z_2|\)</span></p></li>
<li><p><a class="reference external" href="https://www.statisticshowto.datasciencecentral.com/mahalanobis-distance/">Mahalanobis distance</a>
<span class="math notranslate nohighlight">\(r_{1,2}= \sqrt{(p_1 – p_2)^T \; C^{-1} \; (p_1 – p_2)}\)</span>, where <span class="math notranslate nohighlight">\(C\)</span> is the covariance matrix.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import standard stuff, plus material from Scikit Learn</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a synthetic data set and plot it</span>
<span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Feature 2&#39;)
</pre></div>
</div>
<img alt="_images/5530287f4062895c7cd635ddc13df9dddd10e975fe3e379c1ae9ac415b202cc1.png" src="_images/5530287f4062895c7cd635ddc13df9dddd10e975fe3e379c1ae9ac415b202cc1.png" />
</div>
</div>
</section>
<section id="scaling">
<h2>Scaling<a class="headerlink" href="#scaling" title="Link to this heading">#</a></h2>
<p>What if the scaling is different?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a synthetic data set and plot it</span>
<span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X2</span><span class="o">=</span> <span class="n">X</span><span class="o">*</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y2</span><span class="o">=</span><span class="n">y</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X2</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X2</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Feature 2&#39;)
</pre></div>
</div>
<img alt="_images/d25a6502af0adaffcfc383bc1e4e1964419695b51935055e6a14f2e3270baa0b.png" src="_images/d25a6502af0adaffcfc383bc1e4e1964419695b51935055e6a14f2e3270baa0b.png" />
</div>
</div>
<p>The problem is that the distances are dominated by feature1 and feature 2 is not significant, even though intuitively it seems to be very important for classification.</p>
<section id="then-scale-it-first">
<h3>Then scale it first<a class="headerlink" href="#then-scale-it-first" title="Link to this heading">#</a></h3>
<p>To elimninate the distortion due to scaling, normalise the varibles first. Usullay they are normalized so that the means and standard deviations are are the same. Usually <span class="math notranslate nohighlight">\(\bar{x}=0\)</span>¸ and <span class="math notranslate nohighlight">\(\sigma_x =1\)</span>.</p>
<p>$<span class="math notranslate nohighlight">\(x=\frac{x-\bar{x}}{\sigma_x}\)</span><span class="math notranslate nohighlight">\(,
 where \)</span>\bar{x}$ is the mean value of x.</p>
<p>Scikit Learn includes StandardScaler() object for standardisation of features:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">scaler</span><span class="o">=</span><span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X2</span><span class="p">)</span>
<span class="n">X2s</span><span class="o">=</span><span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X2s</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X2s</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Feature 2&#39;)
</pre></div>
</div>
<img alt="_images/6c723410b1b93a1414f29fc17e0a5350e062aea01bbd6d7ff2b436fde2732a1f.png" src="_images/6c723410b1b93a1414f29fc17e0a5350e062aea01bbd6d7ff2b436fde2732a1f.png" />
</div>
</div>
<p>Now both features have equal weights.</p>
<p>But beware outliers when scaling features. Only one outlier can spoil your scaling! There are also robust scalers.</p>
</section>
</section>
<section id="categorial-features">
<h2>Categorial features?<a class="headerlink" href="#categorial-features" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Features can be</p>
<ul>
<li><p>Real values, like floating points or integers, which can be ordered</p></li>
<li><p>Categorical features, which is an unordered set of identifiers for separate classes</p></li>
<li><p>Because you cannot order the categorical features, they do not have distance metrics either</p></li>
<li><p>Many methods rely on distances, and simply using numbers as categories only makes the operation of the predictor worse</p></li>
<li><p>Some methods, like Naive Bayesian Classifier (NBC) can use directly categorical features</p></li>
<li><p>For many others, it is best to use ns <a class="reference internal" href="#(https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)"><span class="xref myst">One Hot Encoding</span></a>, where one binary feature is used to represent each category. Therefore a feature with N-categories will be replaced by a N-bit binary vector.</p></li>
</ul>
</li>
</ul>
<section id="example">
<h3>Example:<a class="headerlink" href="#example" title="Link to this heading">#</a></h3>
<p>Assume that the species of the Iris is in fact a categorical feature for some ML algorithm. If you would code them just simply like ‘setosa’ -&gt; 1, ‘versicolor’ -&gt;2 and ‘virginica’ -&gt; 3, the ML algorithm could think it as a numerical feature, and use it to calculate distances between species :(</p>
<p>The fix is to use so called One Hot Encoding.
Here is the original data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">iris_dataset</span><span class="o">=</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">iris</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">iris_dataset</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">iris</span><span class="o">.</span><span class="n">columns</span><span class="o">=</span><span class="n">iris_dataset</span><span class="o">.</span><span class="n">feature_names</span>
<span class="n">iris</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="n">iris_dataset</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">iris_dataset</span><span class="o">.</span><span class="n">target</span><span class="p">]</span>
<span class="n">iris</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>species</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>setosa</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="one-hot-encoding">
<h3>One hot encoding  <a class="anchor" id="OneHotEncoding"></a><a class="headerlink" href="#one-hot-encoding" title="Link to this heading">#</a></h3>
<p>Now we can encode the species to number in a safe way:</p>
<ol class="arabic simple">
<li><p>Make an encoder and transform the target variable to OneHot format</p></li>
<li><p>The categorial variable can be currently encoded as integers, strings or any objects OneHotEncoder reads it in the for nxp matrix, where n is number of samples and p is the number of features to be encoded.</p></li>
<li><p>Originally target was such kind of numpy array, which do not have the second index at all. It has to be (unfortunately) converted to column vector, which is otherwise the same, but it has also the second axis, which has only one value, a nx1 array.</p></li>
<li><p>This can be done by just simply adding a new dimension into the array, using np.newaxis constant or by using reshape function.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>

<span class="c1"># Change to One hot encoding</span>

<span class="c1"># Add a new column for each species</span>
<span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">iris_dataset</span><span class="o">.</span><span class="n">target_names</span><span class="p">:</span>
    <span class="n">iris</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>
    

<span class="c1"># The result of the One hot encoding is a sparse matrix, which can be converted to numpy </span>
<span class="c1"># array using toarray method:</span>
<span class="n">enc</span><span class="o">=</span><span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">categories</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>
<span class="n">iris</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="s1">&#39;setosa&#39;</span><span class="p">:</span><span class="s1">&#39;virginica&#39;</span><span class="p">]</span><span class="o">=</span><span class="n">enc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">iris_dataset</span><span class="o">.</span><span class="n">target</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="c1">#iris.loc[:,&#39;setosa&#39;:&#39;virginica&#39;]=enc.fit_transform(iris_dataset.target.reshape(150,1)).toarray()</span>
<span class="n">iris</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>species</th>
      <th>setosa</th>
      <th>versicolor</th>
      <th>virginica</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>137</th>
      <td>6.4</td>
      <td>3.1</td>
      <td>5.5</td>
      <td>1.8</td>
      <td>virginica</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>setosa</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>40</th>
      <td>5.0</td>
      <td>3.5</td>
      <td>1.3</td>
      <td>0.3</td>
      <td>setosa</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>138</th>
      <td>6.0</td>
      <td>3.0</td>
      <td>4.8</td>
      <td>1.8</td>
      <td>virginica</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>setosa</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>147</th>
      <td>6.5</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.0</td>
      <td>virginica</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>131</th>
      <td>7.9</td>
      <td>3.8</td>
      <td>6.4</td>
      <td>2.0</td>
      <td>virginica</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>95</th>
      <td>5.7</td>
      <td>3.0</td>
      <td>4.2</td>
      <td>1.2</td>
      <td>versicolor</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>26</th>
      <td>5.0</td>
      <td>3.4</td>
      <td>1.6</td>
      <td>0.4</td>
      <td>setosa</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>92</th>
      <td>5.8</td>
      <td>2.6</td>
      <td>4.0</td>
      <td>1.2</td>
      <td>versicolor</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
</section>
<section id="nearest-neighbours-methods">
<h2>Nearest Neighbours methods <a class="anchor" id="nearestneighbours"></a><a class="headerlink" href="#nearest-neighbours-methods" title="Link to this heading">#</a></h2>
<p>Nearest Neighbour methods provide some very staightforward methods for supervised machine learning</p>
<section id="brute-force-implementation">
<h3>Brute force implementation<a class="headerlink" href="#brute-force-implementation" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Set the number of nearest neighbours, <span class="math notranslate nohighlight">\(K\)</span></p></li>
<li><p>To predict one new sample, calculate its distance to all known training samples</p></li>
<li><p>Order the list of distances</p></li>
<li><p>Select <span class="math notranslate nohighlight">\(K\)</span> nearest samples and use them for prediction</p>
<ul class="simple">
<li><p>In case of classification, the result is the mode of the K-nearest set</p></li>
<li><p>In case of regression, the result is for example the average of the K-nearest set</p></li>
</ul>
</li>
</ol>
<ul class="simple">
<li><p>The asymptotic execution time of the brute for implementation is <span class="math notranslate nohighlight">\(\mathcal{O}[D N^2]\)</span> which makes it unsuitable for large data sets and high dimesional problems</p></li>
<li><p>To extend NN method, the neighbourhood information can be encoded in a tree structure to reduce the number of distances which need to be calculated. For example a KD-Tree implementation can be calculated in <span class="math notranslate nohighlight">\(\mathcal{O}[D N \log ({N})]\)</span> time.</p></li>
<li><p>The Ball-Tree implementation makes algorith even more suitable in high-dimensional problems</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">neighbors</span>
<span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">knn</span><span class="o">=</span><span class="n">neighbors</span><span class="o">.</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="p">)</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([1, 0, 1, 0, 0, 0, 2, 2, 1, 0, 0, 0, 1, 0, 2, 1, 2, 0, 2, 2, 2, 2,
       2, 0, 1, 1, 1, 1, 2, 2, 0, 1, 1, 0, 2, 0, 0, 1, 1, 2, 2, 1, 1, 0,
       0, 0, 1, 1, 2, 2, 0, 1, 0, 1, 2, 2, 1, 1, 0, 1, 1, 2, 2, 2, 2, 1,
       0, 2, 1, 0, 2, 1, 2, 1, 1, 1, 0, 0, 2, 1, 0, 0, 1, 0, 1, 0, 0, 0,
       1, 0, 1, 1, 2, 2, 2, 2, 0, 0, 2, 2])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sklearn.metrics</span> <span class="k">as</span> <span class="nn">metrics</span>
<span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The accuracy of KNN in the original data is..... </span><span class="si">%4.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)))</span>


<span class="n">knn2</span><span class="o">=</span><span class="n">neighbors</span><span class="o">.</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="p">)</span>
<span class="n">knn2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X2s</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The accuracy of KNN in the scaled data is....... </span><span class="si">%4.2f</span><span class="s2">&quot;</span> <span class="o">%</span><span class="k">metrics</span>.accuracy_score(y_true=y, y_pred=knn2.predict(X2s)))


<span class="n">knn3</span><span class="o">=</span><span class="n">neighbors</span><span class="o">.</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="p">)</span>
<span class="n">knn3</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The accuracy of KNN for badly scaled data is.... </span><span class="si">%4.2f</span><span class="s2">&quot;</span> <span class="o">%</span><span class="k">metrics</span>.accuracy_score(y_true=y, y_pred=knn3.predict(X2)))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[31  2  1]
 [ 0 33  0]
 [ 2  0 31]]
The accuracy of KNN in the original data is..... 0.95
The accuracy of KNN in the scaled data is....... 0.96
The accuracy of KNN for badly scaled data is.... 0.75
</pre></div>
</div>
</div>
</div>
</section>
<section id="pipelining">
<h3>Pipelining<a class="headerlink" href="#pipelining" title="Link to this heading">#</a></h3>
<p>In Scikit Learn, all methods are build using the same interface. This makes it easier to build larger machine learning systems by combining different stages together as pipelines.</p>
<p>For example, the scaling of features, dimensionality reduction, and sclassification can be combined as a single pipeline. This is especially usefull, when several datasets (validation data, testing data, production data, etc) needs to be fed through the same stages.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="n">n_neighbors</span><span class="o">=</span><span class="mi">9</span>
<span class="n">pipeline</span><span class="o">=</span><span class="n">Pipeline</span><span class="p">([</span>
        <span class="p">(</span><span class="s1">&#39;Scaling&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
        <span class="p">(</span><span class="s1">&#39;KNN&#39;</span><span class="p">,</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="p">))</span>
    <span class="p">])</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">predictedY</span><span class="o">=</span><span class="n">pipeline</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">predictedY</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">predictedY</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[29  3  2]
 [ 1 32  0]
 [ 2  0 31]]
0.92
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="visualization-of-the-decision-boundaries">
<h2>Visualization of the decision boundaries<a class="headerlink" href="#visualization-of-the-decision-boundaries" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>
<span class="k">def</span> <span class="nf">plotDB</span><span class="p">(</span><span class="n">predictor</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Plots the Decision Boundary</span>
<span class="sd">        pipe = classification pipeline</span>
<span class="sd">        X is the training data used for training the classifier</span>
<span class="sd">        steps = number of x and y steps in calculating the boundary</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Create color map</span>
    <span class="n">cmap_light</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#FFAAAA&#39;</span><span class="p">,</span> <span class="s1">&#39;#AAFFAA&#39;</span><span class="p">,</span> <span class="s1">&#39;#AAAAFF&#39;</span><span class="p">])</span>
    <span class="n">cmap_bold</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#FF0000&#39;</span><span class="p">,</span> <span class="s1">&#39;#00FF00&#39;</span><span class="p">,</span> <span class="s1">&#39;#0000FF&#39;</span><span class="p">])</span>

    <span class="c1"># Plot the decision boundary. For that, we will assign a color to each</span>
    <span class="c1"># point in the mesh [x_min, x_max]x[y_min, y_max].</span>
    <span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">hx</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_max</span> <span class="o">-</span> <span class="n">x_min</span><span class="p">)</span><span class="o">/</span><span class="n">steps</span>
    <span class="n">hy</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_max</span> <span class="o">-</span> <span class="n">y_min</span><span class="p">)</span><span class="o">/</span><span class="n">steps</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="n">hx</span><span class="p">),</span>
                         <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="n">hy</span><span class="p">))</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>

    <span class="c1"># Put the result into a color plot</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">)</span>

    <span class="c1"># Plot also the training points</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_bold</span><span class="p">,</span>
                <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">xx</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">yy</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Decision boundary&quot;</span><span class="p">)</span>
    
<span class="c1"># Display the support vectors of support vector machine</span>
<span class="k">def</span> <span class="nf">DisplaySupportVectors</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">svc</span><span class="p">):</span>
    <span class="n">cmap_bold</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#FF0000&#39;</span><span class="p">,</span> <span class="s1">&#39;#00FF00&#39;</span><span class="p">,</span> <span class="s1">&#39;#0000FF&#39;</span><span class="p">])</span>
    <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;rgb&quot;</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">svc</span><span class="o">.</span><span class="n">support_</span><span class="p">:</span>
        <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">%s</span><span class="s1">x&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">colors</span><span class="p">[</span><span class="n">c</span><span class="p">]),</span> <span class="n">ms</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plotDB</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">y2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/4afbcb7b8ca43641d0458f1ee664e5fdf4a551c7e9478f8b225b6c00f4ca7831.png" src="_images/4afbcb7b8ca43641d0458f1ee664e5fdf4a551c7e9478f8b225b6c00f4ca7831.png" />
</div>
</div>
<section id="variations">
<h3>Variations<a class="headerlink" href="#variations" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Nearest Centroid classifier</p>
<ul>
<li><p>The training data is replaced with a centroid of each class</p></li>
</ul>
</li>
<li><p>Neigborhood Component Analysis (NCA)</p>
<ul>
<li><p>The coordinate axis are changed so that the separation between the classes is maximized</p></li>
<li><p>This supervised dimensionality reduction method can be used for exploring the data</p></li>
<li><p>It can also improve the performance of NN classifiers or regressors</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="nearest-centroid-classifier">
<h2>Nearest Centroid Classifier<a class="headerlink" href="#nearest-centroid-classifier" title="Link to this heading">#</a></h2>
<p>Nearest centroid classifier does not need to store all training data, thats why it is also faster to predict.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors.nearest_centroid</span> <span class="kn">import</span> <span class="n">NearestCentroid</span>
<span class="n">pipelineCentroid</span><span class="o">=</span><span class="n">Pipeline</span><span class="p">([</span>
        <span class="p">(</span><span class="s1">&#39;Scaling&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
        <span class="p">(</span><span class="s1">&#39;KNN&#39;</span><span class="p">,</span> <span class="n">NearestCentroid</span><span class="p">())</span>
    <span class="p">])</span>
<span class="n">pipelineCentroid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X2s</span><span class="p">,</span><span class="n">y2</span><span class="p">)</span>
<span class="n">predictedY</span><span class="o">=</span><span class="n">pipelineCentroid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X2s</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y2</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">predictedY</span><span class="p">))</span>
<span class="n">plotDB</span><span class="p">(</span><span class="n">pipelineCentroid</span><span class="p">,</span> <span class="n">X2s</span><span class="p">,</span> <span class="n">y2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">12</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="kn">from</span> <span class="nn">sklearn.neighbors.nearest_centroid</span> <span class="kn">import</span> <span class="n">NearestCentroid</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="n">pipelineCentroid</span><span class="o">=</span><span class="n">Pipeline</span><span class="p">([</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span>         <span class="p">(</span><span class="s1">&#39;Scaling&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span>         <span class="p">(</span><span class="s1">&#39;KNN&#39;</span><span class="p">,</span> <span class="n">NearestCentroid</span><span class="p">())</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span>     <span class="p">])</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="n">pipelineCentroid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X2s</span><span class="p">,</span><span class="n">y2</span><span class="p">)</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;sklearn.neighbors.nearest_centroid&#39;
</pre></div>
</div>
</div>
</div>
</section>
<section id="support-vector-machine-svm">
<h2>Support Vector Machine (SVM) <a class="anchor" id="supportvectormachine"></a><a class="headerlink" href="#support-vector-machine-svm" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>SVM is kind of Jack of All Trades for classifiers</p></li>
<li><p>It does not save all training samples like NearestNeigbour method, but only the samples near the border of class boundaries.</p></li>
<li><p>These boundary samples are called as support vectors.</p></li>
<li><p>SVM works for high dimensional data and large sample sizes</p></li>
<li><p>Can be used for both classification and regression</p></li>
<li><p>Can be extended to nonlinear decision boundaries using kernels</p></li>
</ul>
<section id="decision-boundary">
<h3>Decision boundary<a class="headerlink" href="#decision-boundary" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>SVM uses samples near the different clusters to define a decision boundary</p></li>
<li><p>The boundary which maximises the marginal of the boundary will be selected</p></li>
<li><p>THe support vectors definind the boundary will be stored</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Lets create a two-dimensional dataset containing two cluster centers</span>
<span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>

<span class="c1"># Now the dataset will be splitted randomly to training set and test set</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Lets plot the data and optimal decision boundary with support vectors</span>
<span class="n">a</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="mf">0.5965</span><span class="p">,</span> <span class="mf">2.33479</span><span class="p">,</span> <span class="mf">0.83645</span><span class="p">,</span> <span class="mf">1.97</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.9567</span><span class="p">,</span> <span class="mf">3.4118</span><span class="p">,</span> <span class="mf">2.11336</span><span class="p">,</span> <span class="mf">2.23518</span><span class="p">],</span>  <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>

<span class="c1"># Plot decision boundaries</span>
<span class="n">m</span><span class="o">=</span><span class="mf">0.15</span><span class="p">;</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">3.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.65</span><span class="p">,</span>   <span class="mf">3.9</span><span class="p">],</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">3.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.65</span><span class="o">+</span><span class="n">m</span><span class="p">,</span> <span class="mf">3.9</span><span class="o">+</span><span class="n">m</span><span class="p">],</span> <span class="s1">&#39;b:&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">3.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.65</span><span class="o">-</span><span class="n">m</span><span class="p">,</span> <span class="mf">3.9</span><span class="o">-</span><span class="n">m</span><span class="p">],</span> <span class="s1">&#39;b:&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">m</span><span class="o">=</span><span class="mf">0.4</span><span class="p">;</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">3.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.3</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">],</span> <span class="s1">&#39;g&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">3.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.3</span><span class="o">+</span><span class="n">m</span><span class="p">,</span> <span class="mf">3.2</span><span class="o">+</span><span class="n">m</span><span class="p">],</span> <span class="s1">&#39;g:&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">3.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.3</span><span class="o">-</span><span class="n">m</span><span class="p">,</span> <span class="mf">3.2</span><span class="o">-</span><span class="n">m</span><span class="p">],</span> <span class="s1">&#39;g:&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x7fbfee757898&gt;]
</pre></div>
</div>
<img alt="_images/39909649b8b2c61972a0275d8efc1319acf13b8961f62c49a4895b5cf0259e41.png" src="_images/39909649b8b2c61972a0275d8efc1319acf13b8961f62c49a4895b5cf0259e41.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Lets now try how actual linear SCV would work</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="n">linsvc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)</span>
<span class="n">linsvc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">plotDB</span><span class="p">(</span><span class="n">linsvc</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accurary in the trainint set..</span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">linsvc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accurary in the test set......</span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">linsvc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">linsvc</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accurary in the trainint set..1.000000
Accurary in the test set......1.000000
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto_deprecated&#39;,
  kernel=&#39;linear&#39;, max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)
</pre></div>
</div>
<img alt="_images/f26676093d2c99924d00c3da2a3e1eaa88915cb0ffa04af6cfd78b36adf7f198.png" src="_images/f26676093d2c99924d00c3da2a3e1eaa88915cb0ffa04af6cfd78b36adf7f198.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Lets try slightly more complex case</span>

<span class="c1"># Lets create a two-dimensional dataset containing three cluster centers</span>
<span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">1.1</span><span class="p">)</span>

<span class="c1"># Now the dataset will be splitted randomly to training set and test set</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Lets now try how actual linear SCV would work</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="n">linsvc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)</span>
<span class="n">linsvc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">plotDB</span><span class="p">(</span><span class="n">linsvc</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accurary in the training set..</span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">linsvc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accurary in the test set......</span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">linsvc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">linsvc</span><span class="p">)</span>

<span class="n">DisplaySupportVectors</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">linsvc</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accurary in the training set..0.940000
Accurary in the test set......0.920000
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto_deprecated&#39;,
  kernel=&#39;linear&#39;, max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)
</pre></div>
</div>
<img alt="_images/bef5386a815f03b9c5b0abbd0dc4f981a9ecf9724a0bb53aaa4bae42cff58b6e.png" src="_images/bef5386a815f03b9c5b0abbd0dc4f981a9ecf9724a0bb53aaa4bae42cff58b6e.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Lets now try how actual linear SCV would work</span>
<span class="n">Linsvc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">()</span>
<span class="n">Linsvc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">plotDB</span><span class="p">(</span><span class="n">Linsvc</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accurary in the trainint set..</span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">Linsvc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accurary in the test set......</span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">Linsvc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Linsvc</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  &quot;the number of iterations.&quot;, ConvergenceWarning)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accurary in the trainint set..0.940000
Accurary in the test set......0.920000
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss=&#39;squared_hinge&#39;, max_iter=1000,
     multi_class=&#39;ovr&#39;, penalty=&#39;l2&#39;, random_state=None, tol=0.0001,
     verbose=0)
</pre></div>
</div>
<img alt="_images/d32e37453b1e3fd72dd08e595b4d3e13e9bc499f6d37eb12a9bbc68803ba2bba.png" src="_images/d32e37453b1e3fd72dd08e595b4d3e13e9bc499f6d37eb12a9bbc68803ba2bba.png" />
</div>
</div>
</section>
</section>
<section id="kernel-svm">
<h2>Kernel SVM <a class="anchor" id="kernelsvm"></a><a class="headerlink" href="#kernel-svm" title="Link to this heading">#</a></h2>
<p>Linear kernel</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Lets now try how actual linear SCV would work</span>
<span class="n">rbfsvc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>  <span class="c1"># gamma &gt; 2 means overfitting, try eg 25 and 0.05</span>
<span class="n">rbfsvc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">plotDB</span><span class="p">(</span><span class="n">rbfsvc</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accurary in the training set..</span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">rbfsvc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accurary in the test set......</span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">rbfsvc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rbfsvc</span><span class="p">)</span>
<span class="n">DisplaySupportVectors</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">rbfsvc</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accurary in the training set..0.940000
Accurary in the test set......0.900000
SVC(C=0.5, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=&#39;ovr&#39;, degree=3, gamma=0.01, kernel=&#39;rbf&#39;,
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)
</pre></div>
</div>
<img alt="_images/710cae9dc1cc07da3c0a347ba89a5d3eaab641938be7b1191b04673a8c7e2f4c.png" src="_images/710cae9dc1cc07da3c0a347ba89a5d3eaab641938be7b1191b04673a8c7e2f4c.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Lets test the model with CV in higher nimensions</span>

<span class="c1"># Lets create a two-dimensional dataset containing three cluster centers</span>
<span class="c1">#X,y=datasets.make_blobs(n_samples=200, centers=5, n_features=3, random_state=0, cluster_std=2)</span>
<span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">1.1</span><span class="p">)</span>

<span class="c1"># Now the dataset will be splitted randomly to training set and test set</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="n">rbfsvc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>  <span class="c1"># gamma &gt; 2 means overfitting</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">rbfsvc</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean CV score is </span><span class="si">%4.2f</span><span class="s2">, all scores=&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()),</span> <span class="n">scores</span><span class="p">)</span>

<span class="c1"># CV can be put into loop to find optimal gamma value</span>
<span class="n">gamma</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mf">1.2</span><span class="p">,</span><span class="mi">40</span><span class="p">)</span>
<span class="n">test_score</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">gamma</span><span class="p">))</span>
<span class="n">train_score</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">gamma</span><span class="p">))</span>
<span class="n">cv_score</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">gamma</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">gamma</span><span class="p">)):</span>
    <span class="n">rbfsvc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">C</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">rbfsvc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">train_score</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">rbfsvc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
    <span class="n">test_score</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">rbfsvc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
    <span class="n">cv_score</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">rbfsvc</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mean CV score is 0.88, all scores= [0.83870968 0.96774194 0.83333333 0.86666667 0.89285714]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">test_score</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Test score&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">train_score</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Train score&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">cv_score</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;CV score&quot;</span><span class="p">)</span>
<span class="n">best_gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">[</span><span class="n">cv_score</span><span class="o">.</span><span class="n">argmax</span><span class="p">()]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best gamma value is </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">best_gamma</span><span class="p">)</span>
<span class="n">rbfsvcbest</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">best_gamma</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Gamma, $\gamma$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accurary in the test set......</span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">rbfsvcbest</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best gamma value is 0.066147
Accurary in the test set......0.980000
</pre></div>
</div>
<img alt="_images/932a73ba5545d7196398d08f5450daa6b31661d3ebd1b29a3fdde9a100323f49.png" src="_images/932a73ba5545d7196398d08f5450daa6b31661d3ebd1b29a3fdde9a100323f49.png" />
</div>
</div>
</section>
<section id="non-linear-classes">
<h2>Non-linear classes<a class="headerlink" href="#non-linear-classes" title="Link to this heading">#</a></h2>
<ul>
<li><p>If the data described by <span class="math notranslate nohighlight">\(p_i=[x_i, y_i]^T\)</span> is not linearly separable, it can be made linearly separable by adding a new term, for example <span class="math notranslate nohighlight">\(z_i=x_i^2 + y_i^2\)</span></p></li>
<li><p>In this case, third dimension is introduced, and the linear classifier can work in the new three dimensional space <span class="math notranslate nohighlight">\( p_i'=[x_i, y_i, z_i]^T \)</span></p></li>
<li><p>SVM uses this kernel trick to separate non-linear cases</p></li>
<li><p>The kernel functions include the dot product of two points in a suitable feature space. Thus defining a notion of similarity, with little computational cost even in very high-dimensional spaces.</p></li>
<li><p>There are many kernel options, most common being</p>
<ul class="simple">
<li><p>Polynomial kernel <span class="math notranslate nohighlight">\(k(p_i, p_j) = (p_i \cdot p_j +1)^d\)</span></p></li>
<li><p>Gaussian  kernel or Gaussian Radial Basis Function (RBF), shown below</p></li>
</ul>
<div class="math notranslate nohighlight">
\[k(p_i, p_j) = \exp \left( - \frac{\Vert p_i-p_j \Vert^2}{2 \sigma^2} \right)
   \qquad 
   k(p_i, p_j) = \exp ( - \gamma \Vert p_i-p_j \Vert^2) \]</div>
</li>
</ul>
<section id="illustration-of-rbf">
<h3>Illustration of RBF<a class="headerlink" href="#illustration-of-rbf" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The following code plots the RBF when <span class="math notranslate nohighlight">\(p_i\)</span> is in origo and <span class="math notranslate nohighlight">\(p_j\)</span> moves along x-axis.</p></li>
<li><p>In real case the RBF is N-dimensional, centered around a sample <span class="math notranslate nohighlight">\(p_i\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the radial basis functions (RBF) with different gamma values</span>
<span class="n">xc</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">1000</span><span class="p">)</span>
<span class="k">for</span> <span class="n">gamma</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">,</span> <span class="p">]:</span>
    <span class="n">r</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">gamma</span><span class="o">*</span><span class="n">xc</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xc</span><span class="p">,</span><span class="n">r</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\gamma$=</span><span class="si">%3.1f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">gamma</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Distance, in x-axis&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$k([0,0,0]^T, [x,0,0]^T)$&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0,0.5,&#39;$k([0,0,0]^T, [x,0,0]^T)$&#39;)
</pre></div>
</div>
<img alt="_images/20a390cfd8f18b7fc1214a4009a43ac21c06b50cfe42f9da3653a24177ff6684.png" src="_images/20a390cfd8f18b7fc1214a4009a43ac21c06b50cfe42f9da3653a24177ff6684.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Test with some example data.</span>
<span class="c1"># How to separate two classes with linear decision function</span>
<span class="c1"># This is one dimensional case, since the second dimension is dummy (only zeros)</span>
<span class="n">x1</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">x2</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
<span class="n">ytest</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
<span class="n">ytest</span><span class="p">[</span><span class="mi">5</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="n">ytest</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.collections.PathCollection at 0x7fbfee7eb358&gt;
</pre></div>
</div>
<img alt="_images/eae275ca9f3dc4b56d7f42b649ed846df9d3b768a5854290e3db606d5627f774.png" src="_images/eae275ca9f3dc4b56d7f42b649ed846df9d3b768a5854290e3db606d5627f774.png" />
</div>
</div>
<p><strong>Solution:</strong> Use the 8th value as a support vector, and use RBF kernel to increase one more dimesion</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import norm, which calculates || p1- p2 ||^2</span>
<span class="kn">from</span> <span class="nn">scipy.linalg</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="c1"># Define the RBF function</span>
<span class="k">def</span> <span class="nf">rbf</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">gamma</span><span class="o">*</span><span class="n">norm</span><span class="p">(</span><span class="n">p1</span><span class="o">-</span><span class="n">p2</span><span class="p">))</span>

<span class="c1"># Calculate the kernel value for all data points</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)):</span>
    <span class="n">x2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">rbf</span><span class="p">(</span><span class="n">x1</span><span class="p">[</span><span class="mi">7</span><span class="p">],</span> <span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mf">0.5</span><span class="p">)</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="n">ytest</span><span class="p">)</span>

<span class="c1"># Mark almost optimal decision boundary as horizontal line</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.lines.Line2D at 0x7fbfebb0c080&gt;
</pre></div>
</div>
<img alt="_images/d88e7281d9c8c210fcabdec7571176b2534d0a37a27a20421a06f8d753832c42.png" src="_images/d88e7281d9c8c210fcabdec7571176b2534d0a37a27a20421a06f8d753832c42.png" />
</div>
</div>
<p>Now the classes are separable, but what is the optimal Gamma value?</p>
</section>
<section id="testing-rbf-in-circular-data">
<h3>Testing RBF in circular data<a class="headerlink" href="#testing-rbf-in-circular-data" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Xc</span><span class="p">,</span><span class="n">yc</span><span class="o">=</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_circles</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">Xc</span><span class="p">,</span><span class="n">yc</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>

<span class="n">rbfsvc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;scale&#39;</span><span class="p">)</span>
<span class="n">rbfsvc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accurary in the training set..</span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">rbfsvc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accurary in the test set......</span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">rbfsvc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rbfsvc</span><span class="p">)</span>
<span class="n">plotDB</span><span class="p">(</span><span class="n">rbfsvc</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">DisplaySupportVectors</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">rbfsvc</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accurary in the training set..1.000000
Accurary in the test set......1.000000
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;scale&#39;, kernel=&#39;rbf&#39;,
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)
</pre></div>
</div>
<img alt="_images/556d6cc8e5985cab45565b6dbaf96ec0eab9dd36cadd71f77d58d1752dd2d367.png" src="_images/556d6cc8e5985cab45565b6dbaf96ec0eab9dd36cadd71f77d58d1752dd2d367.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rbfsvc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">rbfsvc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accurary in the training set..</span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">rbfsvc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accurary in the test set......</span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">rbfsvc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rbfsvc</span><span class="p">)</span>
<span class="n">plotDB</span><span class="p">(</span><span class="n">rbfsvc</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">DisplaySupportVectors</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">rbfsvc</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accurary in the training set..1.000000
Accurary in the test set......1.000000
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=&#39;ovr&#39;, degree=3, gamma=0.5, kernel=&#39;rbf&#39;,
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)
</pre></div>
</div>
<img alt="_images/5a2bdb365fb3ed62f5b9bbbc17072287c856313cf236396a1db40c17d00870da.png" src="_images/5a2bdb365fb3ed62f5b9bbbc17072287c856313cf236396a1db40c17d00870da.png" />
</div>
</div>
<p>Read more from <a class="reference external" href="https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/">Understanding SVM</a></p>
</section>
</section>
<section id="decision-trees-and-forests">
<h2>Decision trees and forests <a class="anchor" id="dtaforests"></a><a class="headerlink" href="#decision-trees-and-forests" title="Link to this heading">#</a></h2>
<section id="decision-tree">
<h3>Decision tree <a class="anchor" id="decisiontrees"></a><a class="headerlink" href="#decision-tree" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>

<span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">1.1</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>

<span class="n">dt</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accurary in the training set..</span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">dt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accurary in the test set......</span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">dt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span>
<span class="n">tree</span><span class="o">.</span><span class="n">export_graphviz</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="s1">&#39;dt.dot&#39;</span><span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rounded</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accurary in the training set..0.913333
Accurary in the test set......0.980000
DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=2,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter=&#39;best&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plotDB</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/33eefa803018f23b80b7d23dedc410b15cd227264aff39fa2b78ccbbdb5e1657.png" src="_images/33eefa803018f23b80b7d23dedc410b15cd227264aff39fa2b78ccbbdb5e1657.png" />
</div>
</div>
</section>
<section id="optimize-the-tree-depth">
<h3>Optimize the tree depth<a class="headerlink" href="#optimize-the-tree-depth" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N</span><span class="o">=</span><span class="mi">6</span>
<span class="n">train_score</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">test_score</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">cv_score</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">depth</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="n">dt</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">depth</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">train_score</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">dt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
    <span class="n">test_score</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">dt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
    <span class="n">cv_score</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">depth</span><span class="p">,</span> <span class="n">train_score</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Train score&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">depth</span><span class="p">,</span> <span class="n">cv_score</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;CV score&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">depth</span><span class="p">,</span> <span class="n">test_score</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Test score&quot;</span><span class="p">)</span>
<span class="n">best_depth</span><span class="o">=</span><span class="n">depth</span><span class="p">[</span><span class="n">cv_score</span><span class="o">.</span><span class="n">argmax</span><span class="p">()]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best depth value is </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">best_depth</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accurary in the test set......</span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">cv_score</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best depth value is 2.000000
Accurary in the test set......0.880462
</pre></div>
</div>
<img alt="_images/c604eee58b019e9da39b8403e3835277273a79bc8958dbfd82ae5633f0540a12.png" src="_images/c604eee58b019e9da39b8403e3835277273a79bc8958dbfd82ae5633f0540a12.png" />
</div>
</div>
<ul class="simple">
<li><p>It seems that two is the optimal depth of the decision tree, since after that the accuracy of the cross validation is not increased any more.</p></li>
<li><p>The accuracy in the training set increases up to 100%, untill every point is in its own leaf, but that is only overfitting to the training data</p></li>
<li><p>Test score shows the same message than cross validation</p></li>
<li><p>Therefore do not use test data in optimising the model. Use it only in the end, when you have selected the optimal model using cross validation</p></li>
</ul>
<p>Red more from <a class="reference external" href="https://scikit-learn.org/stable/modules/tree.html">Skikit Learn</a></p>
</section>
</section>
<section id="ensemble-methods">
<h2>Ensemble methods <a class="anchor" id="ensemblemethods"></a><a class="headerlink" href="#ensemble-methods" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Bagging in <a class="reference external" href="https://scikit-learn.org/stable/modules/ensemble.html">Skikit Learn</a></p>
<ul>
<li><p>A subset of the training data is selected and a full decision trees or other classifier is trained for it</p></li>
<li><p>The output of all predictors in the bag are then aggregated by voting, averaging or other methods</p></li>
<li><p>This method reduces the variance in the predictor generation process by introducing some randomness</p></li>
</ul>
</li>
<li><p>Boosting is another method to combine multiple predictors</p></li>
</ul>
<section id="bagging">
<h3>Bagging <a class="anchor" id="Bagging"></a><a class="headerlink" href="#bagging" title="Link to this heading">#</a></h3>
<section id="randomized-trees">
<h4>Randomized trees   <a class="anchor" id="RandomizedTrees"></a><a class="headerlink" href="#randomized-trees" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Forest of randomized trees is one famous bagging method. it works as follows</p>
<ol class="arabic simple">
<li><p>A random partition of data is drawn from the training data to bootstrap the tree structure</p></li>
<li><p>The tree may use all features or only a random subset of available features</p></li>
<li><p>The output is again aggregated from all predictors</p></li>
<li><p>The two sources of randomness stabilizes the tree structure and reduces overfitting</p></li>
</ol>
</li>
<li><p>Read more from <a class="reference external" href="https://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees">Skikit Learn</a></p></li>
</ul>
</section>
<section id="extratrees-classifier-extremely-randomized-trees">
<h4>Extratrees classifier (Extremely randomized trees)<a class="headerlink" href="#extratrees-classifier-extremely-randomized-trees" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Even more random</p></li>
<li><p>The threshold rules are selected at random for randomly selected features</p></li>
<li><p>The best thresholding rules are voted</p></li>
<li><p>Read more from <a class="reference external" href="https://scikit-learn.org/stable/modules/ensemble.html#extremely-randomized-trees">Skikit Learn</a></p></li>
</ul>
</section>
<section id="optimising-the-parameters-fo-the-extratrees-classifier">
<h4>Optimising the parameters fo the Extratrees classifier<a class="headerlink" href="#optimising-the-parameters-fo-the-extratrees-classifier" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Extratrees classifier has already many more parameters than normal decision tree.</p></li>
<li><p>It is not convenient to try them all to find out an optimal combination</p></li>
<li><p>Hand made optimisation loop with cross validation can be used as shown previously to make exhaustive search</p></li>
<li><p>There is also better method in Scikit Learn, called as <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code></p></li>
<li><p>It uses an optimisation algorithm and CV to find out optimal parameters</p></li>
<li><p>First we just need to define which variables are going to be searched and in which range</p></li>
<li><p>Then we let the optimisation algorith to tune the predictor and we just used the optimal version</p></li>
<li><p>Note that we already got higher accuracy than with using a single tree predictor</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">ExtraTreesClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">1.1</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>

<span class="n">et</span> <span class="o">=</span> <span class="n">ExtraTreesClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="n">et</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Set the parameters by cross-validation</span>
<span class="n">tuned_parameters</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span> <span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span>
                     <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">30</span><span class="p">)}]</span>

<span class="c1">#  Use the GridSearch to find out the best paramers using 5 fold cross validation</span>
<span class="n">tune_et</span><span class="o">=</span><span class="n">GridSearchCV</span><span class="p">(</span><span class="n">et</span><span class="p">,</span> <span class="n">tuned_parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">);</span>
<span class="n">tune_et</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">);</span>
<span class="n">optimal_et</span><span class="o">=</span><span class="n">tune_et</span><span class="o">.</span><span class="n">best_estimator_</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accurary in the training set..</span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">optimal_et</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accurary in the test set......</span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">optimal_et</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">optimal_et</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accurary in the training set..0.933333
Accurary in the test set......0.920000
ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion=&#39;gini&#39;,
           max_depth=3, max_features=&#39;auto&#39;, max_leaf_nodes=None,
           min_impurity_decrease=0.0, min_impurity_split=None,
           min_samples_leaf=1, min_samples_split=2,
           min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=None,
           oob_score=False, random_state=None, verbose=0, warm_start=False)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.
  DeprecationWarning)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plotDB</span><span class="p">(</span><span class="n">optimal_et</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/3c7fd8b43c29660c180b711efe97932bc10a0e14886f949c47041acbe3dd0ec0.png" src="_images/3c7fd8b43c29660c180b711efe97932bc10a0e14886f949c47041acbe3dd0ec0.png" />
</div>
</div>
</section>
</section>
</section>
<section id="boosting">
<h2>Boosting <a class="anchor" id="Boosting"></a><a class="headerlink" href="#boosting" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Construct weak random trees and chain them after each other on modified version of the data</p></li>
<li><p>Examples <a class="reference external" href="https://scikit-learn.org/stable/modules/ensemble.html#adaboost">Adaboost</a>
<a class="reference external" href="https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting">Gradient Tree Boosting</a>
<a class="reference external" href="https://xgboost.readthedocs.io/en/latest/">XGBoost</a></p></li>
</ul>
<section id="adaboost">
<h3>Adaboost <a class="anchor" id="AdaBoost"></a><a class="headerlink" href="#adaboost" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>First create one weak predictor, which is perhaps only slightly better than guessing</p></li>
<li><p>Assign equal weight <span class="math notranslate nohighlight">\(w_i=1/N\)</span> to each of the N samples</p></li>
<li><p>Repeat following boosting iteration M times:</p></li>
<li><p>Try to predict the data with one weak predictor</p></li>
<li><p>Find out which samples were incorrectly classified, and increase their weights, decrease the weights of correctly classified samples</p></li>
<li><p>Train the next predictor with the weighted data so that it concentrates especially to those samples which were difficult for the classifiers this far.</p></li>
<li><p>The final classification result is voted by the predictors</p></li>
</ol>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>First publications</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Freund, Y., &amp; Schapire, R. E. (1997). A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting. Journal of Computer and System Sciences, 55(1), 119–139. <a class="reference external" href="https://doi.org/10.1006/jcss.1997.1504">https://doi.org/10.1006/jcss.1997.1504</a> </p></td>
</tr>
<tr class="row-odd"><td><p>Drucker, H. (1997). Improving Regressors using Boosting Techniques. ICML. </p></td>
</tr>
<tr class="row-even"><td><p>Hastie, T., Rosset, S., Zhu, J., &amp; Zou, H. (2009). Multi-class AdaBoost. Statistics and Its Interface, 2(3), 349–360. <a class="reference external" href="https://doi.org/10.4310/SII.2009.v2.n3.a8">https://doi.org/10.4310/SII.2009.v2.n3.a8</a> </p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="gradient-tree-boosting">
<h3>Gradient Tree Boosting <a class="anchor" id="GBRT"></a><a class="headerlink" href="#gradient-tree-boosting" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Also called as Gradient Boosted Regression Trees (GBRT)</p></li>
<li><p>Improved version of Adaboost</p></li>
<li><p>The predictor is an aggregation of many weak individual predictors, often small decision trees, like in Adaboost</p></li>
<li><p>The main difference is that the boosting in GBRT:s is implemented as an optimisation algorithm</p></li>
</ul>
<div class="math notranslate nohighlight">
\[Obj(\Theta) = \underbrace{L(\Theta)}_{Training Loss} + \underbrace{\Omega(\Theta)}_{Regularization}\]</div>
<div class="math notranslate nohighlight">
\[Obj(\Theta) = \underbrace{L(\Theta)}_{Training Loss} + \underbrace{\Omega(\Theta)}_{Regularization}\]</div>
<ul class="simple">
<li><p>The target of the optimisation is to minimize the Objective function</p></li>
<li><p>The model fitness involves minimization of th training loss and model complexity (Regularization term)</p></li>
<li><p>Training loss function is often a squared distance <span class="math notranslate nohighlight">\(L(\hat{y}_i,y_i) = (\hat{y}_i -y_i)^2\)</span></p></li>
<li><p>Regularization term is usually either</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(L_2\)</span> norm: <span class="math notranslate nohighlight">\(\Omega(w)=\lambda \Vert w \Vert ^2\)</span> or</p></li>
<li><p><span class="math notranslate nohighlight">\(L_1\)</span> norm: <span class="math notranslate nohighlight">\(\Omega(w)=\lambda \vert w \vert\)</span></p></li>
</ul>
</li>
<li><p>Optimizing training loss improves the prediction capability of the model, because it is hoped that the predictor will learn the underlying distributions</p></li>
<li><p>Optimizing regularization encourages simples modes. They tend to predict better for future data, since simpler methods do not suffer from overfitting as easily as complex models</p></li>
<li><p>Read more from Interesting <a class="reference external" href="https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf">slides about gradient boosting</a></p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>First publications</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Breiman, L. (1997). Arcing the edge. Technical Report 486, Statistics Department, University of California at ….</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="extreme-gradient-boosting-xgboost">
<h3>Extreme Gradient Boosting (XGBoost) <a class="anchor" id="XGBoost"></a><a class="headerlink" href="#extreme-gradient-boosting-xgboost" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>A library implementing Gradient Tree Boosting</p></li>
<li><p>Available for many programming languages</p></li>
<li><p>Read more from <a class="reference external" href="https://xgboost.readthedocs.io/en/latest/tutorials/index.html">XGBoost Tutorials</a></p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>First publications</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Friedman, J. H., Hastie, T., &amp; Tibshirani, R. (2000). Additive logistic regression: A statistical view of boosting. <a class="reference external" href="https://doi.org/10.1214/aos/1016218223">https://doi.org/10.1214/aos/1016218223</a> </p></td>
</tr>
<tr class="row-odd"><td><p>Friedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. The Annals of Statistics, 29(5), 1189–1232. <a class="reference external" href="https://doi.org/10.1214/aos/1013203451">https://doi.org/10.1214/aos/1013203451</a> </p></td>
</tr>
<tr class="row-even"><td><p>Friedman, J. H. (2002). Stochastic gradient boosting. Computational Statistics &amp; Data Analysis, 38(4), 367–378.  <a class="reference external" href="https://doi.org/10.1016/S0167-9473(01)00065-2">https://doi.org/10.1016/S0167-9473(01)00065-2</a> </p></td>
</tr>
<tr class="row-odd"><td><p>Chen, T., &amp; Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD ’16, 785–794. <a class="reference external" href="https://doi.org/10.1145/2939672.2939785">https://doi.org/10.1145/2939672.2939785</a> </p></td>
</tr>
</tbody>
</table>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
<span class="n">bt</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">bt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accurary in the training set..</span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">bt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accurary in the test set......</span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">bt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">bt</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accurary in the training set..0.973333
Accurary in the test set......0.960000
GradientBoostingClassifier(criterion=&#39;friedman_mse&#39;, init=None,
              learning_rate=1, loss=&#39;deviance&#39;, max_depth=1,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=9,
              n_iter_no_change=None, presort=&#39;auto&#39;, random_state=0,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plotDB</span><span class="p">(</span><span class="n">bt</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ab98f3ee527cf1e026b9b30c3b8d88d94622736eeb3ec2fbadfebeadab5087d9.png" src="_images/ab98f3ee527cf1e026b9b30c3b8d88d94622736eeb3ec2fbadfebeadab5087d9.png" />
</div>
</div>
</section>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>SVM is good for high dimensional cases</p></li>
<li><p>LinearSVC can include a regularization term L2, or L1</p></li>
<li><p>KernelSVM can form non-linear decision boundaries</p></li>
<li><p>Ensemble methods combine several simple predictors to improve the prediction</p></li>
<li><p>Decision tree is a method to partition the decision space in sections</p></li>
<li><p>Ensemble methods combine several simple predictors, often simple decision trees</p></li>
<li><p>Bagging is a strategy for using many paraller predictors and the aggregation of their results</p></li>
<li><p>Boosting is strategy to implement predictors sequentially so that the successing predictor emphasizes the cases which have been difficult for the previous predictors</p></li>
</ul>
<p>Cons</p>
<ul class="simple">
<li><p>SVM does not work so well for really big data sizes</p></li>
<li><p>It has also problems if there is plenty of noise in the data, so that classes are overlapping</p></li>
</ul>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning">Machine learning <a class="anchor" id="ML"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-target-of-supervised-machine-learning">The target of supervised machine learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-validation-and-testing">Training, Validation and Testing <a class="anchor" id="TrainTestValidate"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-validation">Cross Validation <a class="anchor" id="crossvalidation"></a></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distance-metrics">Distance metrics <a class="anchor" id="distance"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling">Scaling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#then-scale-it-first">Then scale it first</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#categorial-features">Categorial features?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-encoding">One hot encoding  <a class="anchor" id="OneHotEncoding"></a></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nearest-neighbours-methods">Nearest Neighbours methods <a class="anchor" id="nearestneighbours"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#brute-force-implementation">Brute force implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pipelining">Pipelining</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization-of-the-decision-boundaries">Visualization of the decision boundaries</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variations">Variations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nearest-centroid-classifier">Nearest Centroid Classifier</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#support-vector-machine-svm">Support Vector Machine (SVM) <a class="anchor" id="supportvectormachine"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-boundary">Decision boundary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-svm">Kernel SVM <a class="anchor" id="kernelsvm"></a></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#non-linear-classes">Non-linear classes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#illustration-of-rbf">Illustration of RBF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-rbf-in-circular-data">Testing RBF in circular data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-trees-and-forests">Decision trees and forests <a class="anchor" id="dtaforests"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-tree">Decision tree <a class="anchor" id="decisiontrees"></a></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimize-the-tree-depth">Optimize the tree depth</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ensemble-methods">Ensemble methods <a class="anchor" id="ensemblemethods"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bagging">Bagging <a class="anchor" id="Bagging"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#randomized-trees">Randomized trees   <a class="anchor" id="RandomizedTrees"></a></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#extratrees-classifier-extremely-randomized-trees">Extratrees classifier (Extremely randomized trees)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#optimising-the-parameters-fo-the-extratrees-classifier">Optimising the parameters fo the Extratrees classifier</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#boosting">Boosting <a class="anchor" id="Boosting"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adaboost">Adaboost <a class="anchor" id="AdaBoost"></a></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-tree-boosting">Gradient Tree Boosting <a class="anchor" id="GBRT"></a></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#extreme-gradient-boosting-xgboost">Extreme Gradient Boosting (XGBoost) <a class="anchor" id="XGBoost"></a></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Petri Välisuo
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>