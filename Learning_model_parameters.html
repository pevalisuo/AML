
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>12. Model learning strategies &#8212; Applied Machine Learning, 2021</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="18. Natural Language Processing" href="NLP.html" />
    <link rel="prev" title="11. Artificial Neural Networks (ANN)" href="NeuralNetworks.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/ApplesAndOranges.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Applied Machine Learning, 2021</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="practicalities.html">
   About the course
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ReadingAndPlotting.html">
   2. Reading and plotting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Preprocessing_and_feature_extraction.html">
   3. Preprocessing and feature extraction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Subspace_Projections.html">
   4. Dimensionality reduction by Subspace projections
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Clustering.html">
   5. Unsupervised learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="SupervisedMachineLearningTerminology.html">
   6. Supervised machine learning, Terminology
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NearestNeighbors.html">
   7. Nearest Neighbours methods
   <a class="anchor" id="nearestneighbours">
   </a>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="SupportVectorMachine.html">
   8. Support Vector Machine (SVM)
   <a class="anchor" id="supportvectormachine">
   </a>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DecisionTrees.html">
   9. Decision trees and forests
   <a class="anchor" id="dtaforests">
   </a>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Regression.html">
   10. Regression and regularisation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NeuralNetworks.html">
   11. Artificial Neural Networks (ANN)
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   12. Model learning strategies
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NLP.html">
   18. Natural Language Processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="NLP-UWB.html">
   20. Ultra Wide Band positioning literature analysis
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/Learning_model_parameters.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/pevalisuo/AML.git/master?urlpath=tree/book/Learning_model_parameters.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        <a class="jupyterhub-button" href="https://notebooks.csc.fi/#/blueprint/d1fe6e08032e4c17a0f9e0e222414598/hub/user-redirect/git-pull?repo=https://github.com/pevalisuo/AML.git&urlpath=tree/AML.git/book/Learning_model_parameters.ipynb&branch=master"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch JupyterHub" data-toggle="tooltip"
                data-placement="left"><img class="jupyterhub-button-logo"
                    src="_static/images/logo_jupyterhub.svg"
                    alt="Interact on JupyterHub">JupyterHub</button></a>
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   12. Model learning strategies
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#minimization-of-the-error-function">
     12.1. Minimization of the Error function
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loss-functions">
     12.2. Loss functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-descent">
   13. Gradient descent
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stochastic-gradient-descent-sgd">
   14. Stochastic gradient descent (SGD)
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-partial-derivatives-needed-in-the-update-step">
     14.1. The partial derivatives needed in the update step
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-in-neural-networks">
   15. Learning in Neural Networks
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-backpropagation-algorithm">
     15.1. The backpropagation algorithm
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mini-batch">
   16. Mini-batch
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example">
   17. Example
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#partial-learning">
     17.1. Partial learning
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="model-learning-strategies">
<h1><span class="section-number">12. </span>Model learning strategies<a class="headerlink" href="#model-learning-strategies" title="Permalink to this headline">¶</a></h1>
<p>The model optimisation requires</p>
<ol class="simple">
<li><p>Method to evaluate the quality of a current model</p></li>
<li><p>A strategy for improving the model one step better</p></li>
</ol>
<p>Usually the model quality is evaluated by means of a Loss function and optional regularization term. The loss function shows how large error the model makes in prediction and the regularization term shows how complex the model is. The purpose of the optimization is to minimize both the prediction error and complexity of the model. Too complex model may cause overfitting, and therefore regularization is important.</p>
<p>Let <span class="math notranslate nohighlight">\(f()\)</span> be the scoring function which estimates the prediction <span class="math notranslate nohighlight">\(\hat y = f(x_i)\)</span>.</p>
<section id="minimization-of-the-error-function">
<h2><span class="section-number">12.1. </span>Minimization of the Error function<a class="headerlink" href="#minimization-of-the-error-function" title="Permalink to this headline">¶</a></h2>
<p>The general strategy of the model optimisation is then to minimize the regularized training error,  <span class="math notranslate nohighlight">\(E\)</span>, which includes both prediction losses and reqularization term:</p>
<div class="math notranslate nohighlight">
\[
 E(w,b) = \frac{1}{n} \sum_{i=1}^N L\left( y_i, f(x_i) \right) + \alpha R(w)
\]</div>
<p>The regularization coefficient, <span class="math notranslate nohighlight">\(\alpha\)</span>, determines the tradeoff between model complexity and prediction accuracy.</p>
</section>
<section id="loss-functions">
<h2><span class="section-number">12.2. </span>Loss functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">¶</a></h2>
<p>The model leads to different models depending on the loss functions selected and the regularization used. For example the minimization of the error function using <span class="math notranslate nohighlight">\(L_2\)</span> norm without regularization leads to OLS regression model, and with <span class="math notranslate nohighlight">\(L_2\)</span> regularization to ridge regression model.</p>
<p>For example, consider the following two dimensional regression problem:</p>
<div class="math notranslate nohighlight">
\[\hat y = w_1 x_1 + w_2  x_2 + b \]</div>
<p>The <span class="math notranslate nohighlight">\(L_2\)</span> loss function for the problem is:
$<span class="math notranslate nohighlight">\(L=(y-\hat y)^2 = (y - f(x))^2\)</span>$</p>
<p>Without regularization, the loss function can be directly used as error function, and it’s minimization leads to
Linear regression model. By adding <span class="math notranslate nohighlight">\(L_2\)</span> regularization, it leads to ridge regression model, which can both be solved with closed form solutions, but the model can also be found by iterative optimization.</p>
<p>The optimization methods, such as gradient descent method, can be applied indentically to many other loss functions than <span class="math notranslate nohighlight">\(L_2\)</span>-norm, providing and interesting general framework for developing many kinds of machine learning models. The model type will be different when it is optimised with different loss functions and different regularization terms are used.</p>
<p>The most often used loss functions and their relation to the model types are shown in the following table:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>Name</p></th>
<th class="text-align:left head"><p>Loss function</p></th>
<th class="text-align:left head"><p>Type</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>Hinge loss</p></td>
<td class="text-align:left"><p>$<span class="math notranslate nohighlight">\(L(y_i, f(x_i) = max\left(0,1-y_i f(x_i)\right)\)</span>$</p></td>
<td class="text-align:left"><p>Support vector Classification</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Perceptron</p></td>
<td class="text-align:left"><p>$<span class="math notranslate nohighlight">\(L(y_i, f(x_i) = max\left(0,-y_i f(x_i)\right)\)</span>$</p></td>
<td class="text-align:left"><p>ANN</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Huber</p></td>
<td class="text-align:left"><p>$$L(y_i, f(x_i) = \epsilon</p></td>
<td class="text-align:left"><p>y_i-f(x_i)</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Modified Huber</p></td>
<td class="text-align:left"><p>$<span class="math notranslate nohighlight">\(L(y_i, f(x_i) = max\left(0,1-y_i f(x_i)\right)^2, ...\)</span>$</p></td>
<td class="text-align:left"><p>Classification</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Log</p></td>
<td class="text-align:left"><p>$<span class="math notranslate nohighlight">\(L(y_i, f(x_i) = log\left(1+e^{-y_i f(x_i)}\right)\)</span>$</p></td>
<td class="text-align:left"><p>Logistic regression</p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><span class="math notranslate nohighlight">\(L_2\)</span></p></td>
<td class="text-align:left"><p>$<span class="math notranslate nohighlight">\(L(y_i, f(x_i) = \frac{1}{2} \left(y_i,-f(x_i)\right)^2\)</span>$</p></td>
<td class="text-align:left"><p>OLS, Ridge or Lasso</p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>Epsilon insensitive</p></td>
<td class="text-align:left"><p>$$L(y_i, f(x_i) = max\left(0,</p></td>
<td class="text-align:left"><p>y_i,-f(x_i)</p></td>
</tr>
</tbody>
</table>
<p>This means that one common optimisation framework can implement many common machine learning solutions which are often trained with specialized program code.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>

<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">interactive</span><span class="p">,</span> <span class="n">fixed</span><span class="p">,</span> <span class="n">interact_manual</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hinge</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">yh</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span>  <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="o">*</span><span class="n">yh</span><span class="p">)</span>
<span class="n">perceptron</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">yh</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">y</span><span class="o">*</span><span class="n">yh</span><span class="p">)</span>
<span class="n">huber</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">yh</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">e</span><span class="o">=</span><span class="mf">1.35</span><span class="p">:</span> <span class="n">e</span><span class="o">-</span><span class="nb">abs</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">yh</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="n">e</span><span class="o">**</span><span class="mi">2</span>
<span class="n">mhuber</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">yh</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span>  <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="o">*</span><span class="n">yh</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">logreg</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">yh</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">y</span><span class="o">*</span><span class="n">yh</span><span class="p">))</span>
<span class="n">l2norm</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">yh</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">yh</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">l1norm</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">yh</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">yh</span><span class="p">)</span>
<span class="n">epsilon</span><span class="o">=</span> <span class="k">lambda</span> <span class="n">yh</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">e</span><span class="o">=</span><span class="mf">1.35</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">yh</span><span class="p">)</span><span class="o">-</span><span class="n">e</span><span class="p">)</span>

<span class="n">lossfunctions</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;C: Hinge&#39;</span><span class="p">:</span> <span class="n">hinge</span><span class="p">,</span> 
               <span class="s1">&#39;C: Perceptron&#39;</span> <span class="p">:</span> <span class="n">perceptron</span><span class="p">,</span> 
               <span class="s1">&#39;C: Modified Huber&#39;</span><span class="p">:</span> <span class="n">mhuber</span><span class="p">,</span>
               <span class="s1">&#39;C: Logistic&#39;</span> <span class="p">:</span> <span class="n">logreg</span><span class="p">,</span> 
               <span class="s1">&#39;R: Huber&#39;</span><span class="p">:</span> <span class="n">huber</span><span class="p">,</span> 
               <span class="s1">&#39;R: L2&#39;</span> <span class="p">:</span> <span class="n">l2norm</span><span class="p">,</span> 
               <span class="s1">&#39;R: L1&#39;</span> <span class="p">:</span> <span class="n">l1norm</span><span class="p">,</span> 
               <span class="s1">&#39;R: Epsilon insensitive&#39;</span> <span class="p">:</span> <span class="n">epsilon</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yh</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;L2&#39;</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">yh</span><span class="p">,</span> <span class="n">lossfunctions</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">lossfunctions</span><span class="p">[</span><span class="n">loss</span><span class="p">](</span><span class="n">yh</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">yh</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
    
<span class="n">interact</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">loss</span><span class="o">=</span><span class="n">lossfunctions</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "313cabc00c6147819485f9c0f1f0377d"}
</script><div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;function __main__.test(y=0, loss=&#39;L2&#39;)&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span><span class="o">=</span><span class="mi">1</span>
<span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">axc</span><span class="p">,</span> <span class="n">axr</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">lfname</span> <span class="ow">in</span> <span class="n">lossfunctions</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">lfname</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;C&#39;</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">axc</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">axr</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">lossfunctions</span><span class="p">[</span><span class="n">lfname</span><span class="p">](</span><span class="n">yh</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">yh</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">lfname</span><span class="p">)</span>
<span class="n">axc</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axr</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axc</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">axr</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">axc</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\hat y$&#39;</span><span class="p">)</span>
<span class="n">axr</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\hat y$&#39;</span><span class="p">)</span>
<span class="n">axc</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Loss functions for classification&#39;</span><span class="p">)</span>
<span class="n">axr</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Loss functions for regression&#39;</span><span class="p">)</span>
<span class="n">axc</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">],</span> <span class="s1">&#39;k:&#39;</span><span class="p">)</span>
<span class="n">axr</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.5</span><span class="p">],</span> <span class="s1">&#39;k:&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x7fe83e27d220&gt;]
</pre></div>
</div>
<img alt="_images/Learning_model_parameters_6_1.png" src="_images/Learning_model_parameters_6_1.png" />
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="gradient-descent">
<h1><span class="section-number">13. </span>Gradient descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">¶</a></h1>
<p>The Gradient descent is a common optimisation strategy, where the minimal value of the scoring function <span class="math notranslate nohighlight">\(f(x)\)</span> is found by calculating the partial derivatives of <span class="math notranslate nohighlight">\(f()\)</span> by all of its parameters, and updating the model parameters towards the negative gradient.</p>
<p>Gradient descent calculates the gradient using all samples in the training set, which may be rather resource intensive.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="stochastic-gradient-descent-sgd">
<h1><span class="section-number">14. </span>Stochastic gradient descent (SGD)<a class="headerlink" href="#stochastic-gradient-descent-sgd" title="Permalink to this headline">¶</a></h1>
<p>The utilize computational resources more efficiently, the gradient descent method can be modified so that it calculates the gradient and updates the model for every individual sample in the training set. This method provides only noisy estimate of the gradient, and the models is not advancing towards the minimum as directly as the gradient decent, but it usually converges faster than Gradient Descent. Due to this partly stochastic behaviour, this method is called as Stochastic Gradient Descent (SGD).</p>
<p>In SGD the learning of the model is made in the following steps:</p>
<ol class="simple">
<li><p>The training data is shuffled in random order</p></li>
<li><p>Initial values for models parameters, e.g. <span class="math notranslate nohighlight">\(w_1, w_2\)</span> and <span class="math notranslate nohighlight">\(b\)</span> are set. They can be selected randomly or using some educated guess.</p></li>
<li><p>The training data is went through in random order performing the following actions for each sample</p></li>
<li><p>Calculate the loss <span class="math notranslate nohighlight">\(L(w,b)\)</span> of the prediction</p></li>
<li><p>Calculate the complecity <span class="math notranslate nohighlight">\(R(w,b)\)</span> of the prediction</p></li>
<li><p>Update the parameters using the partial derivative of the error function by each model parameter <span class="math notranslate nohighlight">\((w,b)\)</span></p></li>
<li><p>When all samples are went through, reshuffle the data and run the next iteration. If the loss is below a threshold, stop the process and return the trained model</p></li>
</ol>
<section id="the-partial-derivatives-needed-in-the-update-step">
<h2><span class="section-number">14.1. </span>The partial derivatives needed in the update step<a class="headerlink" href="#the-partial-derivatives-needed-in-the-update-step" title="Permalink to this headline">¶</a></h2>
<p>The model parameters are updated using the partial derivatives. If the loss function is the L2 norm, and the learning rate <span class="math notranslate nohighlight">\(\eta \in ]0,1[\)</span>, the new values for <span class="math notranslate nohighlight">\(w_1, w_2\)</span> and <span class="math notranslate nohighlight">\(b\)</span> are calculated as follows:</p>
<div class="math notranslate nohighlight">
\[ w'_1 = w_1 - \eta \frac{\partial L}{\partial w_1} 
= w_1 - \eta \frac{\partial L}{\partial \hat y} 
\cdot \frac{\partial \hat y}{\partial w_1} 
= w_1 - \eta \left(2(\hat y -y) \cdot x_1 \right) \]</div>
<div class="math notranslate nohighlight">
\[ w'_2 = w_2 - \eta \frac{\partial L}{\partial w_2} 
= w_2 - \eta \frac{\partial L}{\partial \hat y} 
\cdot \frac{\partial \hat y}{\partial w_2} 
= w_2 - \eta \left(2(\hat y -y)\cdot x_2 \right) \]</div>
<div class="math notranslate nohighlight">
\[ b' = b - \eta \frac{\partial L}{\partial b}
\cdot \frac{\partial \hat y}{\partial b} 
= b - \eta \left(2(\hat y -y)\cdot 1 \right) \]</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="learning-in-neural-networks">
<h1><span class="section-number">15. </span>Learning in Neural Networks<a class="headerlink" href="#learning-in-neural-networks" title="Permalink to this headline">¶</a></h1>
<section id="the-backpropagation-algorithm">
<h2><span class="section-number">15.1. </span>The backpropagation algorithm<a class="headerlink" href="#the-backpropagation-algorithm" title="Permalink to this headline">¶</a></h2>
<p>When using Stochastic Gradient Descent (<strong>SGD</strong>) training,  the weights, <span class="math notranslate nohighlight">\(w_i\)</span>, are updated towards the gradient (multidimensional derivative) or the loss function.
$<span class="math notranslate nohighlight">\(
    w \leftarrow w - \eta \left(\alpha \frac{\partial R(w)}{\partial w} + \frac{\partial L(w)}{\partial w}\right),
\)</span>$</p>
<p>where <span class="math notranslate nohighlight">\(\eta\)</span> is the learning rate, <span class="math notranslate nohighlight">\(\alpha\)</span> is the regularization term (L2 penalty for exessive model complexity), <span class="math notranslate nohighlight">\(R\)</span> is a function related to model complexity and <span class="math notranslate nohighlight">\(L\)</span> is a loss function. The weights of the model are simply updated to the direction where the model loss is reduced and model complexity is reduced.</p>
<p>Let us use the following perceptron as an example:
<img alt="Perceptron" src="_images/perceptron_sgd.svg" /></p>
<ul class="simple">
<li><p>First the network predicts the output <span class="math notranslate nohighlight">\(\hat{y} = f(x)\)</span> using the current weights <span class="math notranslate nohighlight">\(w\)</span>.</p></li>
<li><p>This prediction is perhaps not accurate but has a prediction error <span class="math notranslate nohighlight">\(y-\hat y\)</span>.</p></li>
<li><p>To make the network better, each coefficient <span class="math notranslate nohighlight">\(w\)</span> will be modified to make the error smaller. To calculate the direction and magnitude of change, the partial derivative of the output by the specific weight is calculated</p></li>
</ul>
<p>To update weight <span class="math notranslate nohighlight">\(w_1\)</span>, we’ll calculate <span class="math notranslate nohighlight">\(\frac{\partial L}{\partial w_1}\)</span>. The chain rule can be used for the calculation. For simplicity, it is assumed that the regularization is not being used.</p>
<p><span class="math notranslate nohighlight">\( S= \bf{w} \cdot \bf{x}\)</span></p>
<p>Sigmoid:  <span class="math notranslate nohighlight">\(\partial a(x) / \partial x = x(1-x)\)</span>
$<span class="math notranslate nohighlight">\(
   \frac{\partial L}{\partial w_1} 
    = \frac{\partial L}{\partial \hat y} \cdot \frac{\partial \hat y}{\partial S} \cdot \frac{\partial S}{\partial w_1}
    = \left(2\cdot (y-\hat y)\right) \cdot \left( S(1-S) \right) \cdot \left( x_1 \right)
\)</span>$</p>
<p>Relu: <span class="math notranslate nohighlight">\(\partial a(x) / \partial x = 1\)</span> (or 0 when <span class="math notranslate nohighlight">\(x \leq 0\)</span>
$<span class="math notranslate nohighlight">\(
   \frac{\partial L}{\partial w_1} 
    = \frac{\partial L}{\partial \hat y} \cdot \frac{\partial \hat y}{\partial S} \cdot \frac{\partial S}{\partial w_1}
    = \left(2\cdot (y-\hat y)\right) \cdot \left( 1 \right) \cdot \left( x_1 \right) = 2 (y-\hat y) x_1
\)</span>$</p>
<p>To update the perceptron using ReLU towards negative gradient for a step <span class="math notranslate nohighlight">\(\eta\)</span> requires that <span class="math notranslate nohighlight">\(w_1\)</span> will be updated as follows:</p>
<div class="math notranslate nohighlight">
\[
    w_1' = w_1 - \eta \cdot 2 (y-\hat y) x_1
\]</div>
<p>If S is negative for a neuron using ReLU, the output and the derivative will be zero, and the neuron is not updated in this run.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="mini-batch">
<h1><span class="section-number">16. </span>Mini-batch<a class="headerlink" href="#mini-batch" title="Permalink to this headline">¶</a></h1>
<p>A mini-batch approach is similar than SGD, but instead of calculating the gradient from only one sample at the time, the mini-batch approach is to calculate the gradient from a small batch of samples at the time.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="example">
<h1><span class="section-number">17. </span>Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">r2_score</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">snippets</span> <span class="kn">import</span> <span class="n">plotDB</span><span class="p">,</span> <span class="n">DisplaySupportVectors</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="n">boston</span><span class="o">=</span><span class="n">load_boston</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/petri/venv/python3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.

    The Boston housing prices dataset has an ethical problem. You can refer to
    the documentation of this function for further details.

    The scikit-learn maintainers therefore strongly discourage the use of this
    dataset unless the purpose of the code is to study and educate about
    ethical issues in data science and machine learning.

    In this special case, you can fetch the dataset from the original
    source::

        import pandas as pd
        import numpy as np


        data_url = &quot;http://lib.stat.cmu.edu/datasets/boston&quot;
        raw_df = pd.read_csv(data_url, sep=&quot;\s+&quot;, skiprows=22, header=None)
        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
        target = raw_df.values[1::2, 2]

    Alternative datasets include the California housing dataset (i.e.
    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing
    dataset. You can load the datasets as follows::

        from sklearn.datasets import fetch_california_housing
        housing = fetch_california_housing()

    for the California housing dataset and::

        from sklearn.datasets import fetch_openml
        housing = fetch_openml(name=&quot;house_prices&quot;, as_frame=True)

    for the Ames housing dataset.
    
  warnings.warn(msg, category=FutureWarning)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>

<span class="c1"># Always scale the input. The most convenient way is to use a pipeline.</span>
<span class="c1"># Default values for the classifier are pretty good, but you can try to change them </span>
<span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span>
                    <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;hinge&#39;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;L2&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">yh_train</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">yh_test</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1">#plt.scatter(X[:,0], X[:,1], c=y, cmap=&#39;rainbow&#39;)</span>
<span class="c1">#plt.xlabel(&#39;Feature 1&#39;)</span>
<span class="c1">#plt.ylabel(&#39;Feature 2&#39;)</span>
<span class="n">plotDB</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy training..&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span><span class="n">yh_train</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy test......&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">yh_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy training.. 0.944
Accuracy test...... 0.896
</pre></div>
</div>
<img alt="_images/Learning_model_parameters_15_1.png" src="_images/Learning_model_parameters_15_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="o">=</span> <span class="n">boston</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="n">y</span><span class="o">=</span> <span class="n">boston</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span>
                    <span class="n">SGDRegressor</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;squared_error&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">))</span>
<span class="o">%</span><span class="k">time</span> model.fit(X_train, y_train)
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of interations needed:&quot;</span><span class="p">,</span> <span class="n">model</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">n_iter_</span><span class="p">)</span>

<span class="c1"># Cross_val_score and score are coefficient of determinations, R^2</span>
<span class="n">RsquaredTR</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">RSquaredTE</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">RSquaredTA</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">y</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">line_kws</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;color&quot;</span><span class="p">:</span> <span class="s2">&quot;red&quot;</span><span class="p">})</span>
<span class="n">RsquaredCV</span><span class="o">=</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;True price&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Predicted price&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Comparison of two-feature model, $R^2$=</span><span class="si">%3.2f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">RsquaredCV</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training score.....&quot;</span><span class="p">,</span> <span class="n">RsquaredTR</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;CV score...........&quot;</span><span class="p">,</span> <span class="n">RsquaredCV</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Testing score......&quot;</span><span class="p">,</span> <span class="n">RSquaredTE</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;All samples score..&quot;</span><span class="p">,</span> <span class="n">RSquaredTA</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 1.84 ms, sys: 537 µs, total: 2.38 ms
Wall time: 2.03 ms
Number of interations needed: 27
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training score..... 0.6942482610909271
CV score........... 0.49277585677707947
Testing score...... 0.7097347354646877
All samples score.. 0.7034289801993874
</pre></div>
</div>
<img alt="_images/Learning_model_parameters_16_2.png" src="_images/Learning_model_parameters_16_2.png" />
</div>
</div>
<section id="partial-learning">
<h2><span class="section-number">17.1. </span>Partial learning<a class="headerlink" href="#partial-learning" title="Permalink to this headline">¶</a></h2>
<p>The SGD modules can be also trained step by step</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Step by step</span>

<span class="n">reg</span> <span class="o">=</span> <span class="n">SGDRegressor</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;squared_error&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;L2&#39;</span><span class="p">)</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_tr</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_te</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">scores_tr</span><span class="o">=</span><span class="p">[]</span>
<span class="n">scores_te</span><span class="o">=</span><span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1700</span><span class="p">):</span>
    <span class="n">reg</span><span class="o">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">scores_tr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
    <span class="n">scores_te</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_te</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>

<span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_te</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>

<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">scores_tr</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">scores_te</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Testing&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1699 0.7172667307025109 0.6995241001156391
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x7fe83b334b20&gt;
</pre></div>
</div>
<img alt="_images/Learning_model_parameters_18_2.png" src="_images/Learning_model_parameters_18_2.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">boston</span><span class="o">.</span><span class="n">feature_names</span><span class="p">[[</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">12</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;RM&#39;, &#39;PTRATIO&#39;, &#39;LSTAT&#39;], dtype=&#39;&lt;U7&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">boston</span><span class="o">.</span><span class="n">feature_names</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;CRIM&#39;, &#39;ZN&#39;, &#39;INDUS&#39;, &#39;CHAS&#39;, &#39;NOX&#39;, &#39;RM&#39;, &#39;AGE&#39;, &#39;DIS&#39;, &#39;RAD&#39;,
       &#39;TAX&#39;, &#39;PTRATIO&#39;, &#39;B&#39;, &#39;LSTAT&#39;], dtype=&#39;&lt;U7&#39;)
</pre></div>
</div>
</div>
</div>
</section>
</section>


<script type="application/vnd.jupyter.widget-state+json">
{"state": {"8387058aad734261900c9f5762ae0d56": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "313cabc00c6147819485f9c0f1f0377d": {"model_name": "VBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": ["widget-interact"], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "VBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "VBoxView", "box_style": "", "children": ["IPY_MODEL_241fbd07ed384388b73f22c775a328f8", "IPY_MODEL_8c57c913acb54c44b6dbe923b5eaf9c4", "IPY_MODEL_9f948ab86d58400a969fcbc3c2e27174"], "layout": "IPY_MODEL_8387058aad734261900c9f5762ae0d56"}}, "488500b8a31140f1a9ddf58c0216ae71": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "4c2b944a356448b7bf8433dc08072d57": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "241fbd07ed384388b73f22c775a328f8": {"model_name": "DropdownModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DropdownModel", "_options_labels": ["1", "-1"], "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "DropdownView", "description": "y", "description_tooltip": null, "disabled": false, "index": 0, "layout": "IPY_MODEL_488500b8a31140f1a9ddf58c0216ae71", "style": "IPY_MODEL_4c2b944a356448b7bf8433dc08072d57"}}, "40980df4cf6f41849582a61b3937f8b0": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "6dea7b2c4476452ba788738294752239": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "8c57c913acb54c44b6dbe923b5eaf9c4": {"model_name": "DropdownModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DropdownModel", "_options_labels": ["C: Hinge", "C: Perceptron", "C: Modified Huber", "C: Logistic", "R: Huber", "R: L2", "R: L1", "R: Epsilon insensitive"], "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "DropdownView", "description": "loss", "description_tooltip": null, "disabled": false, "index": 0, "layout": "IPY_MODEL_40980df4cf6f41849582a61b3937f8b0", "style": "IPY_MODEL_6dea7b2c4476452ba788738294752239"}}, "6a17e54cd40149f6a5fabf9c91820950": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "9f948ab86d58400a969fcbc3c2e27174": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_6a17e54cd40149f6a5fabf9c91820950", "msg_id": "", "outputs": [{"output_type": "display_data", "metadata": {}, "data": {"text/plain": "<Figure size 432x288 with 1 Axes>", "image/png": "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\n"}}]}}}, "version_major": 2, "version_minor": 0}
</script>


    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="NeuralNetworks.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title"><span class="section-number">11. </span>Artificial Neural Networks (ANN)</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="NLP.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title"><span class="section-number">18. </span>Natural Language Processing</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Petri Välisuo<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>